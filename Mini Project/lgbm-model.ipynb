{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67514,"databundleVersionId":7504628,"sourceType":"competition"},{"sourceId":8343521,"sourceType":"datasetVersion","datasetId":4955909},{"sourceId":8342692,"sourceType":"datasetVersion","datasetId":4955280}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.metrics import accuracy_score, hamming_loss\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom skmultilearn.adapt import MLkNN\nfrom nltk.tokenize import TweetTokenizer\nimport string\nimport re\nimport emoji\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-05-07T10:49:57.711766Z","iopub.execute_input":"2024-05-07T10:49:57.712145Z","iopub.status.idle":"2024-05-07T10:49:58.239280Z","shell.execute_reply.started":"2024-05-07T10:49:57.712119Z","shell.execute_reply":"2024-05-07T10:49:58.237732Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/hatespeech/train.csv\n/kaggle/input/hate-speech-detection/sample_submission.csv\n/kaggle/input/hate-speech-detection/train.csv\n/kaggle/input/hate-speech-detection/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/hatespeech/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/hate-speech-detection/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:00.686777Z","iopub.execute_input":"2024-05-07T10:51:00.687177Z","iopub.status.idle":"2024-05-07T10:51:00.753768Z","shell.execute_reply.started":"2024-05-07T10:51:00.687149Z","shell.execute_reply":"2024-05-07T10:51:00.752711Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\n                 ';-)',\n                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\n                 '=)',\n                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\n                 ':>',\n                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\n                 ':-|',\n                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\n\ndef tokenize(tweet):\n    # instantiate the tokenizer class\n    tokenizer = TweetTokenizer(preserve_case=False, \n                              strip_handles=True,\n                              reduce_len=True)\n\n    # tokenize the tweets\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens: # Go through every word in your tokens list\n        if word not in string.punctuation:  # remove punctuation\n            tweets_clean.append(word)\n    result = tweets_clean\n    return \" \".join(result)\n\ndef preprocess(tweet):\n    result = tweet.replace('rt @','@')\n    result = result.replace('@','<user> @')\n    # it will remove hyperlinks\n    result = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '<url>', result)\n\n    # it will remove hashtags. We have to be careful here not to remove \n    # the whole hashtag because text of hashtags contains huge information. \n    # only removing the hash # sign from the word\n    result = re.sub(r'#', '<hashtag>', result)\n\n    # Replace multiple dots with space\n    result = re.sub('\\.\\.+', ' ', result) \n\n    for char in result:\n        if emoji.is_emoji(char):\n            result = result.replace(char, \"<emoticon >\")\n    for emo in emoticons:\n        result = result.replace(emo, \"<emoticon >\")\n\n    result = tokenize(result)\n    # it will remove single numeric terms in the tweet. \n    result = re.sub(r'[0-9]+', '<number>', result)\n    result = re.sub(r'<number>\\s?st', '<number>', result)\n    result = re.sub(r'<number>\\s?nd', '<number>', result)\n    result = re.sub(r'<number>\\s?rd', '<number>', result)\n    result = re.sub(r'<number>\\s?th', '<number>', result)\n\n    return result\n\ndef clean_abstract(text):\n    # Apply the preprocessing steps to the provided text\n    result = preprocess(text.lower())\n    return result","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:02.452395Z","iopub.execute_input":"2024-05-07T10:51:02.452828Z","iopub.status.idle":"2024-05-07T10:51:02.465500Z","shell.execute_reply.started":"2024-05-07T10:51:02.452796Z","shell.execute_reply":"2024-05-07T10:51:02.464353Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(clean_abstract)\n\n# Print the first few rows to verify the changes\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:05.778735Z","iopub.execute_input":"2024-05-07T10:51:05.779515Z","iopub.status.idle":"2024-05-07T10:51:06.929321Z","shell.execute_reply.started":"2024-05-07T10:51:05.779467Z","shell.execute_reply":"2024-05-07T10:51:06.928041Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"     id                                               text  HS  TR  AG\n0  6452  <user> you're the exception you weren't a rude...   1   1   0\n1  4884  if a woman doesn't want you just unleash your ...   1   0   0\n2  1931  son of jamestown protestants that made the usa...   0   0   0\n3  4942  literally just got hit by a car bc this dumb b...   1   1   0\n4  4721  charli fuck you bitch charli omg why am i so e...   1   1   0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom skmultilearn.adapt import MLkNN\nfrom lightgbm import LGBMClassifier\nimport torch\n\n# # Load pre-trained BERTweet tokenizer and model\n# tokenizer = BertTokenizer.from_pretrained('vinai/bertweet-base')\n# model = BertModel.from_pretrained('vinai/bertweet-base')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:09.474588Z","iopub.execute_input":"2024-05-07T10:51:09.475402Z","iopub.status.idle":"2024-05-07T10:51:09.480596Z","shell.execute_reply.started":"2024-05-07T10:51:09.475366Z","shell.execute_reply":"2024-05-07T10:51:09.479540Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.metrics import accuracy_score, hamming_loss\nfrom lightgbm import LGBMClassifier\n\n# Assuming df is your DataFrame containing features and labels\n# Replace X with your features and y with your labels\nX_text = df['text'].tolist()  # Assuming 'text_column' contains the text data\ny=df[[\"HS\", 'TR', 'AG']]  # Assuming 'label1', 'label2', 'label3' are your multilabels\n\ntfidf_vectorizer = TfidfVectorizer(max_features=10000)  # You can adjust max_features as needed\nX_tfidf = tfidf_vectorizer.fit_transform(X_text)\n\n# Step 2: Split the data into train and validation sets (80% train, 20% validation)\nX_train, X_val, y_train, y_val = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:12.025107Z","iopub.execute_input":"2024-05-07T10:51:12.025461Z","iopub.status.idle":"2024-05-07T10:51:12.273408Z","shell.execute_reply.started":"2024-05-07T10:51:12.025435Z","shell.execute_reply":"2024-05-07T10:51:12.272409Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:16.517943Z","iopub.execute_input":"2024-05-07T10:51:16.518339Z","iopub.status.idle":"2024-05-07T10:51:16.524558Z","shell.execute_reply.started":"2024-05-07T10:51:16.518309Z","shell.execute_reply":"2024-05-07T10:51:16.523207Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(4799, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport numpy as np\n\n# Define the number of folds for cross-validation\nnum_folds = 5\n\n# Initialize lists to store evaluation metrics for each fold\naccuracies = []\nhamming_losses = []\n\n# Perform k-fold cross-validation\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\nfor train_index, val_index in kf.split(X_tfidf, y):\n    # Split the data into train and validation sets for this fold\n    X_train_fold, X_val_fold = X_tfidf[train_index], X_tfidf[val_index]\n    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n\n    # Train Binary Relevance classifier with LightGBM\n    classifier = BinaryRelevance(classifier=LGBMClassifier(), require_dense=[False, True])\n    classifier.fit(X_train_fold, y_train_fold)\n\n    # Predict on validation set\n    predictions = classifier.predict(X_val_fold)\n\n    # Evaluate the model\n    accuracy = accuracy_score(y_val_fold, predictions)\n    hamming_loss_val = hamming_loss(y_val_fold, predictions)\n\n    # Store evaluation metrics for this fold\n    accuracies.append(accuracy)\n    hamming_losses.append(hamming_loss_val)\n\n# Calculate average evaluation metrics across all folds\navg_accuracy = np.mean(accuracies)\navg_hamming_loss = np.mean(hamming_losses)\n\nprint(\"Average Accuracy across all folds:\", avg_accuracy)\nprint(\"Average Hamming Loss across all folds:\", avg_hamming_loss)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:21.987238Z","iopub.execute_input":"2024-05-07T10:51:21.987669Z","iopub.status.idle":"2024-05-07T10:51:33.670479Z","shell.execute_reply.started":"2024-05-07T10:51:21.987624Z","shell.execute_reply":"2024-05-07T10:51:33.669221Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 2055, number of negative: 2744\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014671 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19041\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 555\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.428214 -> initscore=-0.289141\n[LightGBM] [Info] Start training from score -0.289141\n[LightGBM] [Info] Number of positive: 745, number of negative: 4054\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011815 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19041\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 555\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.155241 -> initscore=-1.694075\n[LightGBM] [Info] Start training from score -1.694075\n[LightGBM] [Info] Number of positive: 860, number of negative: 3939\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012005 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19041\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 555\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.179204 -> initscore=-1.521750\n[LightGBM] [Info] Start training from score -1.521750\n[LightGBM] [Info] Number of positive: 2007, number of negative: 2792\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012386 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19073\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 559\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.418212 -> initscore=-0.330117\n[LightGBM] [Info] Start training from score -0.330117\n[LightGBM] [Info] Number of positive: 748, number of negative: 4051\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012094 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19073\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 559\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.155866 -> initscore=-1.689316\n[LightGBM] [Info] Start training from score -1.689316\n[LightGBM] [Info] Number of positive: 844, number of negative: 3955\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012201 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19073\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 559\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.175870 -> initscore=-1.544583\n[LightGBM] [Info] Start training from score -1.544583\n[LightGBM] [Info] Number of positive: 2040, number of negative: 2759\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011742 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19124\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 557\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.425089 -> initscore=-0.301918\n[LightGBM] [Info] Start training from score -0.301918\n[LightGBM] [Info] Number of positive: 762, number of negative: 4037\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012012 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19124\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 557\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.158783 -> initscore=-1.667311\n[LightGBM] [Info] Start training from score -1.667311\n[LightGBM] [Info] Number of positive: 844, number of negative: 3955\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012039 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19124\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 557\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.175870 -> initscore=-1.544583\n[LightGBM] [Info] Start training from score -1.544583\n[LightGBM] [Info] Number of positive: 2061, number of negative: 2738\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012304 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19188\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 567\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429464 -> initscore=-0.284036\n[LightGBM] [Info] Start training from score -0.284036\n[LightGBM] [Info] Number of positive: 753, number of negative: 4046\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012293 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19188\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 567\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.156908 -> initscore=-1.681419\n[LightGBM] [Info] Start training from score -1.681419\n[LightGBM] [Info] Number of positive: 848, number of negative: 3951\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012291 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19188\n[LightGBM] [Info] Number of data points in the train set: 4799, number of used features: 567\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176703 -> initscore=-1.538843\n[LightGBM] [Info] Start training from score -1.538843\n[LightGBM] [Info] Number of positive: 2013, number of negative: 2787\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012190 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19157\n[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 570\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419375 -> initscore=-0.325340\n[LightGBM] [Info] Start training from score -0.325340\n[LightGBM] [Info] Number of positive: 752, number of negative: 4048\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012253 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19157\n[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 570\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.156667 -> initscore=-1.683242\n[LightGBM] [Info] Start training from score -1.683242\n[LightGBM] [Info] Number of positive: 860, number of negative: 3940\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012350 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 19157\n[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 570\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.179167 -> initscore=-1.522004\n[LightGBM] [Info] Start training from score -1.522004\nAverage Accuracy across all folds: 0.6321045315540729\nAverage Hamming Loss across all folds: 0.16802789361504958\n","output_type":"stream"}]},{"cell_type":"code","source":"X_test_data = test['text'].tolist()\nX_test_tfidf = tfidf_vectorizer.transform(X_test_data)  # Use transform instead of fit_transform\npredictions_test = classifier.predict(X_test_tfidf)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:42.874403Z","iopub.execute_input":"2024-05-07T10:51:42.874791Z","iopub.status.idle":"2024-05-07T10:51:42.971045Z","shell.execute_reply.started":"2024-05-07T10:51:42.874760Z","shell.execute_reply":"2024-05-07T10:51:42.970110Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(predictions_test)","metadata":{"execution":{"iopub.execute_input":"2024-05-07T10:25:08.405942Z","iopub.status.busy":"2024-05-07T10:25:08.404993Z","iopub.status.idle":"2024-05-07T10:25:08.411846Z","shell.execute_reply":"2024-05-07T10:25:08.410831Z","shell.execute_reply.started":"2024-05-07T10:25:08.405908Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"  (7, 0)\t1\n\n  (8, 0)\t1\n\n  (11, 0)\t1\n\n  (13, 0)\t1\n\n  (14, 0)\t1\n\n  (15, 0)\t1\n\n  (17, 0)\t1\n\n  (19, 0)\t1\n\n  (24, 0)\t1\n\n  (26, 0)\t1\n\n  (29, 0)\t1\n\n  (30, 0)\t1\n\n  (31, 0)\t1\n\n  (33, 0)\t1\n\n  (34, 0)\t1\n\n  (35, 0)\t1\n\n  (37, 0)\t1\n\n  (41, 0)\t1\n\n  (42, 0)\t1\n\n  (44, 0)\t1\n\n  (46, 0)\t1\n\n  (47, 0)\t1\n\n  (48, 0)\t1\n\n  (50, 0)\t1\n\n  (53, 0)\t1\n\n  :\t:\n\n  (1803, 2)\t1\n\n  (1807, 2)\t1\n\n  (1828, 2)\t1\n\n  (1831, 2)\t1\n\n  (1837, 2)\t1\n\n  (1845, 2)\t1\n\n  (1849, 2)\t1\n\n  (1852, 2)\t1\n\n  (1857, 2)\t1\n\n  (1871, 2)\t1\n\n  (1875, 2)\t1\n\n  (1877, 2)\t1\n\n  (1884, 2)\t1\n\n  (1885, 2)\t1\n\n  (1886, 2)\t1\n\n  (1898, 2)\t1\n\n  (1919, 2)\t1\n\n  (1932, 2)\t1\n\n  (1936, 2)\t1\n\n  (1938, 2)\t1\n\n  (1984, 2)\t1\n\n  (1987, 2)\t1\n\n  (1989, 2)\t1\n\n  (1991, 2)\t1\n\n  (1995, 2)\t1\n"}]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(predictions_test.toarray(), columns=y.columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:49.384434Z","iopub.execute_input":"2024-05-07T10:51:49.384856Z","iopub.status.idle":"2024-05-07T10:51:49.390669Z","shell.execute_reply.started":"2024-05-07T10:51:49.384817Z","shell.execute_reply":"2024-05-07T10:51:49.389484Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"predictions_df['id'] = test[\"id\"].values","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:51:58.617694Z","iopub.execute_input":"2024-05-07T10:51:58.618533Z","iopub.status.idle":"2024-05-07T10:51:58.624796Z","shell.execute_reply.started":"2024-05-07T10:51:58.618489Z","shell.execute_reply":"2024-05-07T10:51:58.623595Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T10:52:03.942797Z","iopub.execute_input":"2024-05-07T10:52:03.943597Z","iopub.status.idle":"2024-05-07T10:52:03.953575Z","shell.execute_reply.started":"2024-05-07T10:52:03.943561Z","shell.execute_reply":"2024-05-07T10:52:03.952559Z"},"trusted":true},"execution_count":25,"outputs":[]}]}
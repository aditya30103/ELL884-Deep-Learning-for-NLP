{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67975,"databundleVersionId":7564075,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport random\nimport ast\nimport math\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-07T10:27:22.283884Z","iopub.execute_input":"2024-03-07T10:27:22.284389Z","iopub.status.idle":"2024-03-07T10:27:22.824128Z","shell.execute_reply.started":"2024-03-07T10:27:22.284347Z","shell.execute_reply":"2024-03-07T10:27:22.823243Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/assignment-1-nlp/sample_submission.csv\n/kaggle/input/assignment-1-nlp/test_small.csv\n/kaggle/input/assignment-1-nlp/train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/assignment-1-nlp/train.csv') # loading training data","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:27:22.825839Z","iopub.execute_input":"2024-03-07T10:27:22.826565Z","iopub.status.idle":"2024-03-07T10:27:23.437336Z","shell.execute_reply.started":"2024-03-07T10:27:22.826526Z","shell.execute_reply":"2024-03-07T10:27:23.435852Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"untag_data = []\ntag_data = []\nfor index, row in tqdm(df.iterrows()):\n    # changing data-type of entries from 'str' to 'list'\n    untag_data.append(ast.literal_eval(row['untagged_sentence']))\n    tag_data.append(ast.literal_eval(row['tagged_sentence'])) ","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:27:24.957567Z","iopub.execute_input":"2024-03-07T10:27:24.958026Z","iopub.status.idle":"2024-03-07T10:27:41.778583Z","shell.execute_reply.started":"2024-03-07T10:27:24.957990Z","shell.execute_reply":"2024-03-07T10:27:41.777210Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"47340it [00:16, 2818.19it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/assignment-1-nlp/test_small.csv', index_col=False) # loading test data","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:27:43.805759Z","iopub.execute_input":"2024-03-07T10:27:43.806662Z","iopub.status.idle":"2024-03-07T10:27:43.840433Z","shell.execute_reply.started":"2024-03-07T10:27:43.806597Z","shell.execute_reply":"2024-03-07T10:27:43.838942Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"test_data = {} \nfor index, row in tqdm(test_df.iterrows()):\n    test_data[row['id']] = ast.literal_eval(row['untagged_sentence']) # changing data-type of entries from 'str' to 'list'","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:27:47.046272Z","iopub.execute_input":"2024-03-07T10:27:47.046822Z","iopub.status.idle":"2024-03-07T10:27:47.646023Z","shell.execute_reply.started":"2024-03-07T10:27:47.046774Z","shell.execute_reply":"2024-03-07T10:27:47.644828Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"4000it [00:00, 6822.09it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h2> Constructed Data Structures: </h2>\n\n<font size = \"3\">\n    \n* untag_data (list) : untagged sentences of training set\n    \n* tag_data (list) : tagged sentences of training set\n    \nDictionaries\n    \n* word_tags : maps (word) to dictionary of (tags) and their frequency, 'FREQ' tag stores sum frequencies of all of them i.e count(word)\n\n* tag_count : maps (tag) to the number of times it has occured\n    \n* transition_count: maps (prev_tag, tag) to the number of times it has appeared\n   \n</font>","metadata":{}},{"cell_type":"code","source":"word_tags = {}\ntag_count = defaultdict(int)\ntransition_count = defaultdict(int)\n\n# function for constructing all the necessary data structures\ndef store_tags(tag_count, word_tags, transition_count):    \n    \n    for i in tqdm(range(len(tag_data))):\n        sent = tag_data[i]\n        prev_tag = 'START'\n\n        for word, tag in sent:\n            if word not in word_tags:\n                word_tags[word] = defaultdict(int)\n            word_tags[word][tag] += 1\n            word_tags[word]['FREQ'] += 1\n            tag_count[tag] += 1\n            next_tag = tag\n            transition_count[(prev_tag, next_tag)] += 1\n            prev_tag = next_tag\n\n        next_tag = 'END'\n        transition_count[(prev_tag, next_tag)] += 1\n        \n        # insert \"OOV\" tag\n        word_tags[\"OOV\"] = defaultdict(int)\n    return\n    \nstore_tags(tag_count, word_tags, transition_count)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:27:50.300779Z","iopub.execute_input":"2024-03-07T10:27:50.301327Z","iopub.status.idle":"2024-03-07T10:27:52.486234Z","shell.execute_reply.started":"2024-03-07T10:27:50.301276Z","shell.execute_reply":"2024-03-07T10:27:52.484733Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 47340/47340 [00:02<00:00, 21843.37it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h3> Constructed Maps: </h3>\n<font size = \"3\"> \n    \n* state_map : maps (tag) to corresponding state index <br>\n    \n* invert_state : maps (state index) to corresponding tag\n    \n* observation_map : maps (unique words) to corresponding word index\n    \n* invert_observation : maps (word index) to corresponding unique word \n</font>","metadata":{}},{"cell_type":"code","source":"# Map Constructions\n\n# Global Variables\nN = len(tag_count)   # 49 dist tags in training set: # of states of HMM\nK = len(word_tags)   # total distinct words in corpus \n\nstate_map = {tag[0]: i for i, tag in enumerate(tag_count.items())}\ninvert_state = {value: key for key, value in state_map.items()}\nobservation_map = {obser: i for i, obser in enumerate(word_tags)}\ninvert_observation = {value: key for key, value in observation_map.items()}","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:00.362225Z","iopub.execute_input":"2024-03-07T10:28:00.362757Z","iopub.status.idle":"2024-03-07T10:28:00.424872Z","shell.execute_reply.started":"2024-03-07T10:28:00.362714Z","shell.execute_reply":"2024-03-07T10:28:00.423437Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"<h2>HMM Implementation:</h2>\n","metadata":{}},{"cell_type":"code","source":"# A, B Initialisation\n# PI : 'START' -> () transition probabilities\n# Laplace smoothing for all\n\nPI = np.zeros(N, dtype=np.float32)\nA = np.zeros((N, N), dtype=np.float32)\nB = np.zeros((N, K), dtype=np.float32)\n\ndef pi_init(PI, transition_count, invert_state, sm):\n    total_sent = len(tag_data)\n    for i in tqdm(range(N)):\n        num = transition_count[('START', invert_state[i])]\n        denom = total_sent\n        PI[i] = (num + sm)/(denom + sm*N)\n    return PI\n        \n    \ndef transition_init(A, tag_count, transition_count, invert_state, sm):    # with smoothing \n    for i in tqdm(range(N)):\n        denom = tag_count[invert_state[i]]\n        for j in range(N):\n            num = transition_count[(invert_state[i], invert_state[j])]\n            A[i][j] = (num + sm)/(denom + sm*N)\n    return A\n\n# bj(vk) : probability of an observation vk being generated from a state qj\ndef emission_init(B, KAY, word_tags, invert_state, invert_observation, tag_count, sm):\n    for i in tqdm(range(N)):\n        denom = tag_count[invert_state[i]]\n        for k in range(K):\n            if invert_observation[k] == \"OOV\":\n                B[i][k] = KAY[i]\n            else:\n                num = word_tags[invert_observation[k]][invert_state[i]]\n                B[i][k] = (num + sm)/(denom + sm*K)\n    return B\n\ndef k_bag(word_tags, state_map, invert_state, sm):\n    dict_k = defaultdict(int)\n    tot_k = 0\n    for word, tagdict in tqdm(word_tags.items()):\n        if (tagdict['FREQ']) <= 5:   # K = 5\n            for key, freq in tagdict.items():\n                if key != 'FREQ' and freq != 0:\n                    tot_k += 1\n                    if key not in dict_k:\n                        dict_k[key] = 1\n                    else:\n                        dict_k[key] += freq\n                        \n    kay = np.zeros(N, dtype=np.float32)\n    for i in range(N):\n        tag = invert_state[i]\n        kay[i] = (dict_k[tag] + sm)/(tot_k + sm*N)\n    return kay\n    \n# Tune Hyperparameters\nKAY = k_bag(word_tags, state_map, invert_state, 0.001)\nA = transition_init(A, tag_count, transition_count, invert_state, 0.001)\nB = emission_init(B, KAY, word_tags, invert_state, invert_observation, tag_count, 0.00001)\nPI = pi_init(PI, transition_count, invert_state, 0.001)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:14.669295Z","iopub.execute_input":"2024-03-07T10:28:14.669818Z","iopub.status.idle":"2024-03-07T10:28:20.200908Z","shell.execute_reply.started":"2024-03-07T10:28:14.669776Z","shell.execute_reply":"2024-03-07T10:28:20.199821Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 51209/51209 [00:00<00:00, 510883.41it/s]\n100%|██████████| 49/49 [00:00<00:00, 9428.86it/s]\n100%|██████████| 49/49 [00:05<00:00,  9.10it/s]\n100%|██████████| 49/49 [00:00<00:00, 200900.19it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h4>Print A, B</h4>","metadata":{}},{"cell_type":"code","source":"start_row, end_row = 0, 48\nstart_col, end_col = 0, 10\n\n# Create lists for rows and columns based on the given range\nrows = [invert_state[i] for i in range(start_row, end_row + 1)]\ncols = [invert_state[j] for j in range(start_col, end_col + 1)]\n\n# Ensure that the lengths of rows and cols match the shape of the A matrix\nassert len(rows) == end_row - start_row + 1, \"Mismatch in the number of rows\"\nassert len(cols) == end_col - start_col + 1, \"Mismatch in the number of columns\"\n\n# Create a DataFrame with the specified rows and columns from the subset of the A matrix\na_sub = pd.DataFrame(A[start_row:end_row + 1, start_col:end_col + 1], index=rows, columns=cols)\n\n# Print the subset DataFrame\n# print(a_sub)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:23.756311Z","iopub.execute_input":"2024-03-07T10:28:23.756838Z","iopub.status.idle":"2024-03-07T10:28:23.766282Z","shell.execute_reply.started":"2024-03-07T10:28:23.756794Z","shell.execute_reply":"2024-03-07T10:28:23.765205Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"start_row, end_row = 0, 48\nstart_col, end_col = 0, 4\n\nrows = [invert_state[i] for i in range(start_row, end_row + 1)]\ncols = [invert_observation[j] for j in range(start_col, end_col + 1)]\n    \nb_sub = pd.DataFrame(B[start_row:end_row + 1, start_col:end_col + 1], index=rows, columns=cols)\n# print(b_sub)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:26.431888Z","iopub.execute_input":"2024-03-07T10:28:26.432385Z","iopub.status.idle":"2024-03-07T10:28:26.440115Z","shell.execute_reply.started":"2024-03-07T10:28:26.432343Z","shell.execute_reply":"2024-03-07T10:28:26.439039Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<h3> HMM Training </h3>","metadata":{}},{"cell_type":"markdown","source":"<h4> Baum Welch Algorithm </h4>","metadata":{}},{"cell_type":"code","source":"# Computes alpha, beta matrix via forward backward algorithm\n\ndef for_back(sentence, T, A, B, PI):\n    # Initialize alpha and beta matrices\n    alpha = np.zeros((N, T))\n    beta = np.zeros((N, T))\n\n    # Alphas - Forward pass\n    observation_prob = B[:, observation_map[sentence[0]]]  # Precompute observation probabilities\n    alpha[:, 0] = PI * observation_prob \n\n    for t in range(1, T):\n        observation_prob = B[:, observation_map[sentence[t]]]  # Precompute observation probabilities\n        alpha[:, t] = np.dot(alpha[:, t-1], A) * observation_prob\n\n    # Betas - Backward pass\n    beta[:, T-1] = 1\n        \n    # Betas - Backward pass\n    for t in range(T-2, -1, -1):\n        observation_prob = B[:, observation_map[sentence[t+1]]]  # Precompute observation probabilities\n        beta[:, t] = np.dot(A, observation_prob * beta[:, t+1])\n    \n    # Alpha and Beta termination\n    observation_alpha = np.sum(alpha[:, T-1])\n    observation_beta = np.sum(beta[:, 0] * B[:, observation_map[sentence[0]]] * PI)\n\n    assert np.isclose(observation_alpha, observation_beta)  # Check for equality\n    obs_prob = observation_alpha\n\n    return alpha, beta, obs_prob","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:37.377084Z","iopub.execute_input":"2024-03-07T10:28:37.377572Z","iopub.status.idle":"2024-03-07T10:28:37.389614Z","shell.execute_reply.started":"2024-03-07T10:28:37.377535Z","shell.execute_reply":"2024-03-07T10:28:37.388243Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Expectation Step\n\ndef expectation(sentence, T, alpha, beta, obs_prob):\n    gamma = np.zeros((N, T))\n    eta = np.zeros((N, N, T-1))\n\n    # gamma\n    gamma[:, :] = alpha * beta / obs_prob\n\n    # eta\n    for i in range(N):\n        for j in range(N):\n            word_next = np.array([observation_map[word] for word in sentence[1:]], dtype = int)  # Convert list to NumPy array\n            eta[i, j, :] = (alpha[i, :-1] * A[i, j] * B[j, word_next] * beta[j, 1:]) / obs_prob\n\n    return gamma, eta","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:39.631763Z","iopub.execute_input":"2024-03-07T10:28:39.632251Z","iopub.status.idle":"2024-03-07T10:28:39.641002Z","shell.execute_reply.started":"2024-03-07T10:28:39.632212Z","shell.execute_reply":"2024-03-07T10:28:39.639698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Maximisation Step\n\ndef maximisation(T, sentence, gamma, eta, PI):\n    \n    new_A = np.zeros((N, N))\n    new_B = np.zeros((N, K))\n    \n    new_PI = np.zeros(N)\n    \n    # new_PI\n    new_PI = gamma[:, 0]\n    \n    # Convert the list of words to an array of indices using observation_map\n    word_indices = np.array([observation_map[word] for word in sentence], dtype = int)\n    \n    # new_A\n    for i in range(N):\n        for j in range(N):\n            \n            num = np.sum(eta[i, j, :])  # Sum along the third axis (dimension T-1)\n            denom = np.sum(eta[i, :, :])  # Sum along the second axis (dimension N)\n            \n            if denom == 0:\n                new_A[i, j] = 0\n            else:\n                new_A[i, j] = num / denom\n    \n    # new_B\n    for j in range(N):\n        for k in range(K):\n            \n            num = np.sum(gamma[j, (word_indices == k)])\n            denom = np.sum(gamma[j, :])\n            \n            if denom == 0:\n                new_B[j, k] = 0\n            else:\n                new_B[j, k] = num / denom\n    \n    return new_A, new_B, new_PI","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:44.469933Z","iopub.execute_input":"2024-03-07T10:28:44.470452Z","iopub.status.idle":"2024-03-07T10:28:44.482831Z","shell.execute_reply.started":"2024-03-07T10:28:44.470406Z","shell.execute_reply":"2024-03-07T10:28:44.481125Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# HMM Learning Wrapper Function\n\ndef hmm_bw(sentence, A, B, PI):\n    T = len(sentence)\n    alpha, beta, prob = for_back(sentence, T, A, B, PI)\n    gamma, eta = expectation(sentence, T, alpha, beta, prob)\n    A, B, PI = maximisation(T, sentence, gamma, eta, PI)\n    return A, B, PI","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:52.550793Z","iopub.execute_input":"2024-03-07T10:28:52.551357Z","iopub.status.idle":"2024-03-07T10:28:52.559548Z","shell.execute_reply.started":"2024-03-07T10:28:52.551312Z","shell.execute_reply":"2024-03-07T10:28:52.558215Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<h4> HMM Decode </h4>","metadata":{}},{"cell_type":"code","source":"def viterbi(untagged_sentence, AV, BV, PIV):\n    \n    T = len(untagged_sentence)   # length of untagged sentence\n    \n    vit = np.zeros((N, T), dtype = np.float32) # N states\n    backpointer = np.zeros((N, T), dtype = int)\n    \n    epsilon = 1e-18\n    \n    # Calculate the initial probabilities outside the loop\n    log_pi = np.log(PIV + epsilon)\n    \n    # Initialisation\n    word_zero = untagged_sentence[0]\n    for j in range(N):\n        vit[j, 0] = log_pi[j] + np.log(BV[j, observation_map[word_zero]] + epsilon)\n        backpointer[j, 0] = -1\n    \n    # Recurrence\n    for t in range(1, T):\n        word_t = untagged_sentence[t]\n\n        for j in range(N):                \n            probs = vit[:, t-1] + np.log(AV[:, j] + epsilon) + math.log(BV[j, observation_map[word_t]] + epsilon)\n            bestprob = np.argmax(probs)\n            \n            backpointer[j][t] = bestprob\n            vit[j][t] = probs[bestprob]\n        \n    # Backtrack v1   \n    bestpathpointer = np.argmax(vit[:, T-1])\n    tags = []\n    for t in range(T-1, -1, -1):\n        tags.append((untagged_sentence[t], invert_state[bestpathpointer]))\n        bestpathpointer = backpointer[bestpathpointer][t]\n    \n    return tags[::-1]\n\n#     Print Viterbi Matrix\n#     df_vit = pd.DataFrame(vit, columns=[untagged_sentence[i] for i in range(T)], index=[invert_state[i] for i in range(N)])\n#     print(\"Viterbi Matrix (vit):\")\n#     print(df_vit)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:28:55.890696Z","iopub.execute_input":"2024-03-07T10:28:55.891237Z","iopub.status.idle":"2024-03-07T10:28:55.904624Z","shell.execute_reply.started":"2024-03-07T10:28:55.891195Z","shell.execute_reply":"2024-03-07T10:28:55.903581Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<h4> HMM Tagger </h4>","metadata":{}},{"cell_type":"code","source":"def hmm_tagger_util(sent_id, untagged_sentence): \n    oov_dict = {}\n    \n    for i in range(len(untagged_sentence)):\n        if untagged_sentence[i] not in observation_map:\n            oov_dict[i] = untagged_sentence[i]\n            untagged_sentence[i] = \"OOV\"\n    \n    # NO OOV CASE - direct decode\n    if len(oov_dict) == 0:\n        tagged_sentence = viterbi(untagged_sentence, A, B, PI)\n        store_submission(sent_id, tagged_sentence)\n        return\n            \n    # MORE THAN ONE OOV CASE - direct decode\n    if len(oov_dict) > 1:\n        tagged_sentence = viterbi(untagged_sentence, A, B, PI)\n        \n        # Replacing former values\n        for idx, word in oov_dict.items():\n            tag = tagged_sentence[idx][1]\n            tagged_sentence[idx] = (word, tag)\n        \n        store_submission(sent_id, tagged_sentence)\n        return\n    \n    # ONE OOV CASE - decode followed by learning for ~ 40% cases\n    else:\n        \n        probability_distribution = [0.4, 0.6] # YES, NO\n        learn = random.choices([\"yes\", \"no\"], weights=probability_distribution, k=1)[0]\n        \n        if(learn == \"no\"):\n            tagged_sentence = viterbi(untagged_sentence, A, B, PI)\n        \n            # Replacing former values\n            for idx, word in oov_dict.items():\n                tag = tagged_sentence[idx][1]\n                tagged_sentence[idx] = (word, tag)\n\n            store_submission(sent_id, tagged_sentence)\n            return\n        \n        # EM LEARNING!\n        else:\n            AV = np.zeros(A.shape)\n            BV = np.zeros(B.shape)\n            PIV = np.zeros(PI.shape)\n            \n            AV, BV, PIV = hmm_bw(untagged_sentence, A, B, PI)\n            tagged_sentence = viterbi(untagged_sentence, AV, BV, PIV)\n            store_submission(sent_id, tagged_sentence)\n            return","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:29:18.106713Z","iopub.execute_input":"2024-03-07T10:29:18.107273Z","iopub.status.idle":"2024-03-07T10:29:18.122247Z","shell.execute_reply.started":"2024-03-07T10:29:18.107232Z","shell.execute_reply":"2024-03-07T10:29:18.120570Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<h4> HMM Accuracy Prediction on Train Set: </h4>\n","metadata":{}},{"cell_type":"code","source":"def compare_decode(index):\n    t = len(untag_data[index])\n    hmm_tag = viterbi(untag_data[index])\n    \n    miss = 0\n    for i in range(t):\n        if tag_data[index][i][1] != hmm_tag[i][1]:\n            miss += 1\n            print([tag_data[index][i][0], tag_data[index][i][1], hmm_tag[i][1]])\n    return miss","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:29:22.012726Z","iopub.execute_input":"2024-03-07T10:29:22.013282Z","iopub.status.idle":"2024-03-07T10:29:22.021736Z","shell.execute_reply.started":"2024-03-07T10:29:22.013236Z","shell.execute_reply":"2024-03-07T10:29:22.020378Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# wrong = 0\n# num = 0\n# s = 0\n# acc = 0\n# for sent_idx in tqdm(range(3030,3060)):\n#     num = len(untag_data[sent_idx])\n#     wrong = compare_decode(sent_idx)\n#     acc += ((num - wrong)/(num))*100\n#     s += 1\n    \n# train_accuracy = acc/s\n# print(train_accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:29:23.833197Z","iopub.execute_input":"2024-03-07T10:29:23.833669Z","iopub.status.idle":"2024-03-07T10:29:23.839169Z","shell.execute_reply.started":"2024-03-07T10:29:23.833635Z","shell.execute_reply":"2024-03-07T10:29:23.837874Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<h2>MEMM Implementation:</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass MEMM:\n    \n    def __init__(self):\n        self.data = []\n        self.state_map = {}\n        self.invert_state = {}\n        self.observation_map = {}\n        self.invert_observation = {}\n        self.N = 0\n        self.K = 0\n        self.alpha = 0.01\n        self.logP = np.empty((1,1))\n        \n    def load(self, data):\n        self.data = data\n        \n    def load_dictionaries(self, state_map, invert_state, observation_map, invert_observation):\n        self.state_map = state_map\n        self.invert_state = invert_state\n        self.observation_map = observation_map\n        self.invert_observation = invert_observation\n        self.N = len(state_map)\n        self.K = len(observation_map)\n        \n    def memm_main(self):\n        \n        word_features = np.zeros((self.N + 2, self.K))\n        \n        for sent in self.data:\n            prev_tag = 'START'\n            for i in range(len(sent)):\n                if i == 0:\n                    word_features[0, self.observation_map[sent[i][0]]] += 1\n                else:\n                    word_features[self.state_map[prev_tag], self.observation_map[sent[i][0]]] += 1\n                if i == len(sent) - 1:\n                    word_features[self.N + 1, self.observation_map[sent[i][0]]] += 1\n                prev_tag = sent[i][1]\n        \n        w = np.random.rand(self.N, self.N + 2)\n    \n        for epoch in range(5):\n            log_p = np.zeros((self.N, self.K))\n\n            for word_ind in range(self.K):\n                denom = 0\n                for state_ind in range(self.N):\n                    log_p[state_ind, word_ind] = np.dot(w[state_ind, :] , word_features[:, word_ind])  # Corrected\n                    denom += np.exp(np.dot(w[state_ind, :] , word_features[:, word_ind]))  # Corrected\n                log_p[:, word_ind] = log_p[:, word_ind] - np.log(denom)  # Corrected\n\n            grad = np.zeros((self.N, self.N + 2))\n\n            for word in range(self.K):\n                for i in range(self.N):\n                    for j in range(self.N + 2):\n                        grad[i, j] += -(1 - np.exp(log_p[i, word])) * word_features[j, word]  # Corrected\n\n            grad = self.alpha * grad\n\n            w = w - grad\n        \n        self.logP = np.zeros((self.N, self.K))\n        \n        for word_ind in range(self.K):\n            denom = 0\n            for state_ind in range(self.N):\n                self.logP[state_ind, word_ind] = np.dot(w[state_ind, :] , word_features[:, word_ind])  # Corrected\n                denom += np.exp(np.dot(w[state_ind, :] , word_features[:, word_ind]))  # Corrected\n            self.logP[:, word_ind] = self.logP[:, word_ind] - np.log(denom)  # Corrected\n    \n    def evaluate(self, untagged_sentence):\n        \n        T = len(untagged_sentence)\n        \n        vit = np.zeros((self.N, T), dtype=np.float32)  # N states\n        backpointer = np.zeros((self.N, T), dtype=int)\n        \n        epsilon = 1e-18\n\n        # Initialization\n        word_zero = untagged_sentence[0]\n        for j in range(self.N):  # Corrected\n            vit[j, 0] = self.logP[j, self.observation_map[word_zero]]\n            backpointer[j, 0] = -1\n\n        # Recurrence\n        for t in range(1, T):\n            word_t = untagged_sentence[t]\n\n            for j in range(self.N):  # Corrected\n                probs = vit[:, t-1] + self.logP[j, self.observation_map[word_t]]\n                bestprob = np.argmax(probs)\n\n                backpointer[j, t] = bestprob\n                vit[j, t] = probs[bestprob]\n\n        # Backtrack v1   \n        bestpathpointer = np.argmax(vit[:, T-1])\n        tags = []\n        for t in range(T-1, -1, -1):\n            tags.append((untagged_sentence[t], self.invert_state[bestpathpointer]))  # Corrected\n            bestpathpointer = backpointer[bestpathpointer, t]\n\n        tagged_sentence = tags[::-1]  # Reversed the tags list to get the correct order\n        return tagged_sentence","metadata":{"execution":{"iopub.status.busy":"2024-03-07T10:29:30.238011Z","iopub.execute_input":"2024-03-07T10:29:30.238521Z","iopub.status.idle":"2024-03-07T10:29:30.270793Z","shell.execute_reply.started":"2024-03-07T10:29:30.238485Z","shell.execute_reply":"2024-03-07T10:29:30.269257Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"<h4> MEMM Tagger </h4>","metadata":{}},{"cell_type":"code","source":"memm = MEMM()\nmemm.load(tag_data)\nmemm.load_dictionaries(state_map, invert_state, observation_map, invert_observation)\nmemm.memm_main()\ndef memm_tagger_util(sent_id, untagged_sentence):\n    tagged_sentence = memm.evaluate(untagged_sentence)\n    store_submission(sent_id, tagged_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Random Tagger:</h3>","metadata":{}},{"cell_type":"code","source":"# cell to implement tagger that allots random tags to words in a sentence\n\ndef random_tagger_util(sent_id, untagged_sentence):\n    if(sent_id in list(submission['id'])):\n        return\n    tagged_sentence = []\n    for word in untagged_sentence:\n        tagged_sentence.append((word, random.choice(distinct_tags)))\n    store_submission(sent_id, tagged_sentence)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:34:24.789408Z","iopub.execute_input":"2024-02-10T16:34:24.789829Z","iopub.status.idle":"2024-02-10T16:34:24.799075Z","shell.execute_reply.started":"2024-02-10T16:34:24.789800Z","shell.execute_reply":"2024-02-10T16:34:24.795313Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"<h2>Submission:</h2>","metadata":{}},{"cell_type":"code","source":"submission = {'id': [], 'tagged_sentence' : []} # dictionary to store tag predictions\n# NOTE ---> ensure that tagged_sentence's corresponing 'id' is same as 'id' of corresponding 'untagged_sentence' in training data\ndef store_submission(sent_id, tagged_sentence):\n    \n    global submission\n    submission['id'].append(sent_id)\n    submission['tagged_sentence'].append(tagged_sentence)\n    \ndef clear_submission():\n    global submission\n    submission = {'id': [], 'tagged_sentence' : []}","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:34:25.512289Z","iopub.execute_input":"2024-02-10T16:34:25.512741Z","iopub.status.idle":"2024-02-10T16:34:25.519702Z","shell.execute_reply.started":"2024-02-10T16:34:25.512703Z","shell.execute_reply":"2024-02-10T16:34:25.518408Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"<h4>Investigating Unknown Words in Test Set</h4>","metadata":{}},{"cell_type":"code","source":"# Investigating Unknown Words in Test Set\n\n# count = 0\n# tot = 0\n# dict_t = {}\n# for sent_id in tqdm(list(test_data.keys())):\n#     sent = test_data[sent_id]\n#     s = 0\n#     for word in sent:\n#         tot += 1\n#         if word_tags.get(word) == None:\n#             s += 1\n#             count += 1\n#             if dict_t.get(word) == None:\n#                 dict_t[word] = 1\n#             else:\n#                 dict_t[word] += 1\n#     if s == 1:\n#         print(sent_id)\n#         break\n\n# print(count/tot)\n# print(max(dict_t.values()))\n# print(min(dict_t.values()))","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:34:40.568294Z","iopub.execute_input":"2024-02-10T16:34:40.568715Z","iopub.status.idle":"2024-02-10T16:34:40.574347Z","shell.execute_reply.started":"2024-02-10T16:34:40.568685Z","shell.execute_reply":"2024-02-10T16:34:40.573182Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# AV = np.zeros(A.shape)\n# BV = np.zeros(B.shape)\n# PIV = np.zeros(PI.shape)\n\n# for epoch in tqdm(range(1)):\n    \n#     untagged_sentence = test_data[15]\n\n#     oov_dict = {}\n\n#     for i in range(len(untagged_sentence)):\n#         if untagged_sentence[i] not in observation_map:\n#             oov_dict[i] = untagged_sentence[i]\n#             untagged_sentence[i] = \"OOV\"\n\n#     AV, BV, PIV = hmm_bw(untagged_sentence, A, B, PI)\n#     tagged_sentence = viterbi(untagged_sentence, AV, BV, PIV)\n\n#     # Replacing former values\n#     for idx, word in oov_dict.items():\n#         tag = tagged_sentence[idx][1]\n#         tagged_sentence[idx] = (word, tag)\n\n#     print(tagged_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-02-10T16:34:40.866307Z","iopub.execute_input":"2024-02-10T16:34:40.866707Z","iopub.status.idle":"2024-02-10T16:34:40.872540Z","shell.execute_reply.started":"2024-02-10T16:34:40.866675Z","shell.execute_reply":"2024-02-10T16:34:40.871402Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"for sent_id in tqdm(list(test_data.keys())):\n    sent = test_data[sent_id]\n    hmm_tagger_util(sent_id, sent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_directory = '/kaggle/working/'\npd.DataFrame(submission).to_csv(path_to_directory +' sample_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T09:32:22.253249Z","iopub.execute_input":"2024-02-07T09:32:22.253671Z","iopub.status.idle":"2024-02-07T09:32:22.325227Z","shell.execute_reply.started":"2024-02-07T09:32:22.253639Z","shell.execute_reply":"2024-02-07T09:32:22.323977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans=pd.read_csv(\"/kaggle/input/assignment-1-nlp/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}